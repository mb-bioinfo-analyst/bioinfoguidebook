{
  "hash": "4cc6537faad133d35bb49a64f8e054fe",
  "result": {
    "markdown": "---\ntitle: \"Introduction to ANOVA and Linear Models\"\nauthor: \"Bilal Mustafa\"\ndate: \"2024-08-15\"\ncategories: [Statistics, Data Analysis, R Programming, Statistical Methods, ANOVA, Linear Regression, Machine Learning, Data Science, Quantitative Analysis, Statistical Modeling]\nimage: \"image.jpg\"\n---\n\n\n# Introduction:\n\nIt is very important to know how different groups compare and how factors affect each other when you are doing data analysis.\nOne of the most important statistical methods we can use to test these differences and interactions is Analysis of Variance (ANOVA).\nThis post will explain what ANOVA and Linear Models are and how to use them in R.\nYou will also get to use real datasets to practice.\n\n------------------------------------------------------------------------\n\n# Who Should Read This:\n\nThis blog post is for people who are new to or already know a little about R and want to learn how to do and understand ANOVA and Linear Models.\nThis guide will give you useful skills to improve your data analysis skills, no matter if you are a student, researcher, or data scientist.\n\n------------------------------------------------------------------------\n\n# A Brief Look at ANOVA\n\n## What is the ANOVA?\n\nOne way to use statistics is to compare the means of three or more groups and see if at least one of them is significantly different from the others.\nThis is called analysis of variance (ANOVA).\nA lot of people use it in business, biology, and the social sciences.\n\n------------------------------------------------------------------------\n\n## Types of ANOVA:\n\n1.  With one-way ANOVA, you can see if there are any differences in the means of three or more groups that are not linked to each other.\\\n2.  Two-Way ANOVA looks at how two independent factors affect a dependent variable, taking into account the effects that happen when the variables interact.\\\n3.  Measurements Taken More Than Once ANOVA is used to compare two or more sets of data from the same subject in different ways.\\\n4.  Multiple dependent variables can be used in multivariate ANOVA (MANOVA), which lets you test the effects on a single result.\\\n\n## ANOVA: Why Use It?\n\nWhen you want to find out if different conditions, treatments, or interventions lead to different results, ANOVA is very helpful.\nIt gives a statistical way to figure out if changes in data are caused by real effects or by random variation.\n\n------------------------------------------------------------------------\n\n## One-way ANOVA\n\nThe One-Way ANOVA checks if there are statistically significant changes between the means of three or more separate groups that are not related to each other.\n\n### Structure:\n\n-   You can think of an independent variable as a category variable with two or more levels, like Fertilizer A, B, and C.\\\n-   Dependent Variable: An result variable that changes over time, like plant growth or test scores.\\\n\nLet's say you want to see how three different fertilizers (Fertilizer A, B, and C) affect plant growth (in centimeters).\nANOVA helps you figure out if these fertilizers make a big difference in the growth.\\\n\n[Important Points:]{style=\"color:red;\"}\\\n\n-   The null hypothesis (H₀) says that all group means are the same.\\\n-   The other idea is that the mean of at least one group is not the same.\\\n-   We reject the null hypothesis if the p-value is less than the significance level, which is usually 0.05. This means that there is a significant difference between the groups.\\\n\n### What ANOVA Is Based On (Basic Assumptions)\n\nSome conditions must be met before ANOVA can be performed:\n\n-   Random and Unrelated Observations: The data should come from groups that were chosen at random and are unrelated to each other.\\\n-   Normality: The data in each group should be spread out in a way that is similar to the normal distribution. This assumption is not as important if you have a large sample size.\\\n-   The differences between the groups should be about the same. This is called homogeneity of variation.\\\n\n**Note:** The assumptions of normality and homogeneity of variance can be loosened (made lenient) a bit when sample size is big.\n\n------------------------------------------------------------------------\n\n### ANOVA Example Case Study\n\nLet's use the PlantGrowth dataset to show an example.\nThis dataset has data on how plants grew in different treatment groups.\nWe want to know if the plant growth in these groups is very different from that in the other groups.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the tidyverse package\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n```{.r .cell-code}\n# Load the PlantGrowth dataset\ndata(\"PlantGrowth\")\n\n# View the first few rows of the dataset\nhead(PlantGrowth)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  weight group\n1   4.17  ctrl\n2   5.58  ctrl\n3   5.18  ctrl\n4   6.11  ctrl\n5   4.50  ctrl\n6   4.61  ctrl\n```\n:::\n:::\n\n\nFirst, we load the tidyverse package, which has tools for working with and showing data.\nNext, we load the PlantGrowth dataset to start looking into it.\n\n#### Data Exploration\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Summary statistics for the dataset\nsummary(PlantGrowth)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     weight       group   \n Min.   :3.590   ctrl:10  \n 1st Qu.:4.550   trt1:10  \n Median :5.155   trt2:10  \n Mean   :5.073            \n 3rd Qu.:5.530            \n Max.   :6.310            \n```\n:::\n\n```{.r .cell-code}\n# Check the structure of the dataset\nstr(PlantGrowth)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t30 obs. of  2 variables:\n $ weight: num  4.17 5.58 5.18 6.11 4.5 4.61 5.17 4.53 5.33 5.14 ...\n $ group : Factor w/ 3 levels \"ctrl\",\"trt1\",..: 1 1 1 1 1 1 1 1 1 1 ...\n```\n:::\n\n```{.r .cell-code}\n# Count the number of observations in each group\nPlantGrowth %>%\n  group_by(group) %>%\n  summarise(count = n())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 2\n  group count\n  <fct> <int>\n1 ctrl     10\n2 trt1     10\n3 trt2     10\n```\n:::\n:::\n\n\nTo get a sense of the whole dataset, we use simple methods like summary() and str().\nSorting the data by treatment group helps us see how it is spread out across the different groups.\n\n#### Visualization\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Boxplot of plant weight by group\nPlantGrowth %>%\n  ggplot(aes(x = group, y = weight, fill = group)) +\n  geom_boxplot() +\n  stat_summary(\n    fun = mean,\n    geom = \"point\",\n    shape = 23,\n    size = 3,\n    color = \"black\",\n    fill = \"white\"\n  ) +\n  labs(title = \"Plant Weight by Treatment Group\",\n       x = \"Treatment Group\",\n       y = \"Weight\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](anova_lm_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nThis boxplot shows how the plant weights changed in the different treatment groups.\nFind places where the boxes meet or clear gaps between them to see how the groups compare.\n\n#### Testing Assumptions\n\n**Equality of Variances**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bartlet Test of homogeneity of variances\nbartlett.test(weight ~ group, data = PlantGrowth)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tBartlett test of homogeneity of variances\n\ndata:  weight by group\nBartlett's K-squared = 2.8786, df = 2, p-value = 0.2371\n```\n:::\n:::\n\n\nThe Bartlett test checks to see if the differences between groups are the same.\nIf the p-value is not significant, it means that the assumption of homogeneity of differences is true.\n\n**Normality of the Data**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Shapiro-Wilk test for each group\nby(PlantGrowth$weight, PlantGrowth$group, shapiro.test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPlantGrowth$group: ctrl\n\n\tShapiro-Wilk normality test\n\ndata:  dd[x, ]\nW = 0.95668, p-value = 0.7475\n\n------------------------------------------------------------ \nPlantGrowth$group: trt1\n\n\tShapiro-Wilk normality test\n\ndata:  dd[x, ]\nW = 0.93041, p-value = 0.4519\n\n------------------------------------------------------------ \nPlantGrowth$group: trt2\n\n\tShapiro-Wilk normality test\n\ndata:  dd[x, ]\nW = 0.94101, p-value = 0.5643\n```\n:::\n:::\n\n\nTo see if the data in each group is normal, the Shapiro-Wilk test is used.\nIf the p-value is more than 0.05, it means that the data is probably pretty normal.\n\n#### ANOVA in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Perform ANOVA\nanova_result <- aov(weight ~ group, data = PlantGrowth)\n\n# Display the ANOVA table\nsummary(anova_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value Pr(>F)  \ngroup        2  3.766  1.8832   4.846 0.0159 *\nResiduals   27 10.492  0.3886                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThere is an ANOVA table that shows the F-statistic and the p-value.\nIf the p-value is less than 0.05, it means that there is a substantial distinction in the plant weights between the treatment groups.\n\n#### Post-Hoc Tests (Tukey's HSD)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Perform Tukey's HSD Test\ntukey_result <- TukeyHSD(anova_result)\n\n# Display the Tukey HSD result\ntukey_result\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = weight ~ group, data = PlantGrowth)\n\n$group\n            diff        lwr       upr     p adj\ntrt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711\ntrt2-ctrl  0.494 -0.1972161 1.1852161 0.1979960\ntrt2-trt1  0.865  0.1737839 1.5562161 0.0120064\n```\n:::\n:::\n\n\n#### Visualization\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize Tukey's HSD results\nplot(tukey_result)\n```\n\n::: {.cell-output-display}\n![](anova_lm_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nIf the ANOVA shows that there are significant differences, Tukey's HSD test helps figure out which groups are different.\nThe plot shows these similarities graphically by showing which pairs of groups are very different from each other.\n\n------------------------------------------------------------------------\n\n## ANOVA vs. Linear Modeling\n\n### ANOVA\n\n-   Purpose: Compares group means to test if there are significant differences.\\\n-   Limitations: Focuses only on categorical independent variables.\\\n\n### Linear Modeling\n\n-   Purpose: Examines the relationship between the dependent variable and multiple predictors (both categorical and continuous).\\\n-   Advantages: Provides detailed estimates of the effects and allows for more flexibility in analysis.\\\n\n### Side-by-Side Comparison\n\n| Aspect        | ANOVA                                  | Linear Modeling                                  |\n|---------------|----------------------------------------|--------------------------------------------------|\n| **Question**  | Are the group means different?         | How does the outcome change with each predictor? |\n| **Variables** | Categorical independent variables only | Both categorical and continuous predictors       |\n| **Output**    | F-statistic, p-value                   | Coefficients, p-values, R²                       |\n\n## Analysis with Linear Modeling\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit a linear model\nlm_result <- lm(weight ~ group, data = PlantGrowth)\n\n# Display the summary of the linear model\nsummary(lm_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = weight ~ group, data = PlantGrowth)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0710 -0.4180 -0.0060  0.2627  1.3690 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   5.0320     0.1971  25.527   <2e-16 ***\ngrouptrt1    -0.3710     0.2788  -1.331   0.1944    \ngrouptrt2     0.4940     0.2788   1.772   0.0877 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6234 on 27 degrees of freedom\nMultiple R-squared:  0.2641,\tAdjusted R-squared:  0.2096 \nF-statistic: 4.846 on 2 and 27 DF,  p-value: 0.01591\n```\n:::\n:::\n\n\nThe linear model gives you factors that show how each treatment group changed the plant weight, along with p-values that you can use to see how important these changes are.\n\n### Diagnostic Plots\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Diagnostic plots for the linear model\npar(mfrow = c(2, 2))  # Arrange plots in a 2x2 grid\nplot(lm_result)\n```\n\n::: {.cell-output-display}\n![](anova_lm_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nYou can use diagnostic plots to make sure that the linear model's assumptions are met, like that the residuals are normal and that the model is linear.\nYou might not be able to trust the model's results if these assumptions are voilated.\n\n------------------------------------------------------------------------\n\n### Conclusion\n\n-   ANOVA helps determine if there are significant differences between group means.\\\n-   Linear Modeling provides more flexibility and detailed information about the relationships between variables.\\\n\nUse your own samples to test these methods.\nTry these statistical methods out in different situations and see how they can help you find insights in your data.\n\n------------------------------------------------------------------------\n\n# Resources & References:\n\n-   Davies, T. M. (2016). The Book of R: A First Course in Programming and Statistics. San Francisco: No Starch Press.\\\n-   R for Data Science - Comprehensive guide to using R for data analysis.\\\n-   Statistics with R Specialization - Coursera course for learning statistics using R. [External Link](https://www.coursera.org/specializations/statistics)\\\n\nHave questions or feedback?\nLeave a comment below or reach out to us on social media.\nWe're here to help you on your data analysis journey!\n\n------------------------------------------------------------------------\n",
    "supporting": [
      "anova_lm_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}